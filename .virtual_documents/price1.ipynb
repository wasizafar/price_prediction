#from google.colab import drive
#drive.mount('/content/drive')


import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np
from scipy import stats
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt
import seaborn as sns
import joblib


# 1. Data Preprocessing
#def preprocess_data(data_path):
# Load the dataset
data_path = 'flipkart_com-ecommerce_sample.csv'
df = pd.read_csv(data_path)

# Select only the relevant features
df = df[['product_name', 'product_category_tree', 'retail_price', 'discounted_price', 'overall_rating', 'brand']]

# Fill missing values in 'overall_rating' with the mean
df['overall_rating'] = pd.to_numeric(df['overall_rating'], errors='coerce')  # Convert to numeric, coercing errors

# Modify 'product_category_tree' to select the first category before ">>"
df['product_category_tree'] = df['product_category_tree'].str.split('>>').str[0].str.strip()

# Handle missing values in 'brand' and 'product_name'
df['brand'] = df['brand'].fillna('Unknown')
df.fillna({'brand':'unknown'}, inplace = True)
# Fill missing values in product_name and overall_rating
df.fillna({'product_name': 'unknown'},inplace= True)
df.fillna({'overall_rating': df['overall_rating'].mean()}, inplace=True)
df.fillna({'retail_price':0}, inplace = True)
df.fillna({'discounted_price':0}, inplace=True)


## Apply Label Encoding for 'product_category_tree' and 'brand'
#label_encoders = {}
#for col in ['product_category_tree', 'brand']:
#    le = LabelEncoder()
#    df[col] = le.fit_transform(df[col])
#    label_encoders[col] = le

# Return the processed dataframe and the label encoders



# 2. Exploratory Data Analysis (EDA)
#def eda(df):
# Summary of the dataset
print(df.describe())

# Check for missing values
print("\nMissing values in each column:\n", df.isnull().sum())

# Plot the distribution of 'retail_price' and 'discounted_price'
plt.figure(figsize=(12, 6))
sns.histplot(df['retail_price'], bins=50, kde=True, color='blue')
plt.title('Distribution of Retail Price')
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(df['discounted_price'], bins=50, kde=True, color='green')
plt.title('Distribution of Discounted Price')
plt.show()

# Correlation matrix
plt.figure(figsize=(10, 6))

# Select only numerical features for correlation analysis
numerical_df = df.select_dtypes(include=['number'])

sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()


# 3. Remove outliers using the Z-score method
#def remove_outliers(df):
z_scores = np.abs(stats.zscore(df[['retail_price', 'discounted_price', 'overall_rating']]))
df_cleaned = df[(z_scores < 3).all(axis=1)]
#return df_cleaned


# 4. Feature Engineering and Model Creation
#def feature_engineering_and_model(df):
# Preprocess the data
#df = preprocess_data(df)

# Target and features
X = df[['product_name', 'product_category_tree', 'retail_price', 'overall_rating', 'brand']]
y = df['discounted_price']

# Handle text data using TfidfVectorizer, ensuring no stop word removal
tfidf_vectorizer_name = TfidfVectorizer(max_features=50, stop_words=None)  # Disable stop word removal
# try:
product_name_tfidf = tfidf_vectorizer_name.fit_transform(X['product_name']).toarray()
# except ValueError as e:
#     print(f"Error during TF-IDF vectorization: {e}")
#     return None, None, None, None, None

# Apply Label Encoding for 'product_category_tree' and 'brand'
label_encoders = {}
for col in ['product_category_tree', 'brand']:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Combine the features (TF-IDF and numeric features)
X_combined = np.concatenate([product_name_tfidf, X[['retail_price', 'product_category_tree', 'overall_rating', 'brand']].values], axis=1)

# Scale the numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_combined)

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Create the model
model = Sequential([
    Dense(256, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),
    Dropout(0.4),  # Increased dropout rate
    BatchNormalization(),
    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.4),  # Increased dropout rate
    BatchNormalization(),
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.4),  # Increased dropout rate
    BatchNormalization(),
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.4),  # Increased dropout rate
    BatchNormalization(),
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.4),  # Increased dropout rate
    BatchNormalization(),
    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.4),
    BatchNormalization(),
    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.4),
    BatchNormalization(),
    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.4),
    Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')


# Train the model with early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=500, batch_size=16, validation_data=(X_val, y_val))

#return model, history, scaler, tfidf_vectorizer_name, label_encoders


# 5. Main function to run everything
#def main(data_path):
# Preprocess the data
#df = preprocess_data(data_path) # Pass the data_path to preprocess_data function

# EDA
#eda(df)

# Remove outliers
#df_cleaned = remove_outliers(df)

# Feature engineering and model training
#model, history, scaler, tfidf_vectorizer_name, label_encoders = feature_engineering_and_model(df_cleaned)

# Save the model and preprocessing objects
model.save('discounted_price_model.h5')
joblib.dump(scaler, 'model/scaler.pkl')
joblib.dump(tfidf_vectorizer_name, 'tfidf_vectorizer_name.pkl')
joblib.dump(label_encoders, 'label_encoders.pkl')


#if __name__ == "__main__":
#    data_path = '/content/drive/MyDrive/class IA Project/flipkart_com-ecommerce_sample.csv'  # Replace with your CSV file path
#    main(data_path)



